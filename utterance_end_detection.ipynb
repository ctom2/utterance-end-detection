{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-of-speech classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # for reading the .json files\n",
    "import json # for reading the .json files (dialog dataset)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MultiWOZ-Parser; reading the names of the files for training, testing, validation datasets\n",
    "# https://github.com/jojonki/MultiWOZ-Parser/blob/master/parser.py\n",
    "\n",
    "def load_json(data_file):\n",
    "    if os.path.isfile(data_file):\n",
    "        with open(data_file, 'r') as read_file:\n",
    "            data = json.load(read_file)\n",
    "            return data\n",
    "\n",
    "def load_list_file(list_file):\n",
    "    with open(list_file, 'r') as read_file:\n",
    "        dialog_id_list = read_file.readlines()\n",
    "        dialog_id_list = [l.strip('\\n') for l in dialog_id_list]\n",
    "        return dialog_id_list\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_data_file = '../MULTIWOZ/data.json'\n",
    "dialog_data = load_json(dialog_data_file)\n",
    "dialog_id_list = list(set(dialog_data.keys()))\n",
    "\n",
    "valid_list_file = '../MULTIWOZ/valListFile.json'\n",
    "test_list_file = '../MULTIWOZ/testListFile.json'\n",
    "\n",
    "valid_id_list = list(set(load_list_file(valid_list_file)))\n",
    "test_id_list = load_list_file(test_list_file)\n",
    "train_id_list = [did for did in dialog_id_list if did not in (valid_id_list + test_id_list)]\n",
    "\n",
    "train_data = [v for k, v in dialog_data.items() if k in train_id_list]\n",
    "valid_data = [v for k, v in dialog_data.items() if k in valid_id_list]\n",
    "test_data = [v for k, v in dialog_data.items() if k in test_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all datasets together\n",
    "data = train_data + valid_data + test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # for regex\n",
    "\n",
    "def text_tokens(text):\n",
    "    # transforming to lowercase\n",
    "    text = text.lower()\n",
    "    # replacing whitespace characters with spaces\n",
    "    text = re.sub(\"\\\\s\", \" \", text)\n",
    "    # removing everything that is not a letter\n",
    "    text = re.sub(\"[^a-zA-Z ']\", \"\", text)\n",
    "    \n",
    "    # splitting string into array based on spaces\n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    # removing empty strings from the tokens array\n",
    "    tokens = list(filter(('').__ne__, tokens))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MultiWOZ-Parser\n",
    "# https://github.com/jojonki/MultiWOZ-Parser/blob/master/parser.py\n",
    "\n",
    "def get_dst_diff(prev_d, crnt_d):\n",
    "    assert len(prev_d) == len(crnt_d)\n",
    "    diff = {}\n",
    "    for ((k1, v1), (k2, v2)) in zip(prev_d.items(), crnt_d.items()):\n",
    "        assert k1 == k2\n",
    "        if v1 != v2: # updated\n",
    "            diff[k2] = v2\n",
    "    return diff\n",
    "\n",
    "def get_lines(d):\n",
    "    assert 'log' in d\n",
    "    assert 'goal' in d\n",
    "    domains = []\n",
    "    ignore_keys_in_goal = ['eod', 'messageLen', 'message'] # eod (probably) means the user archieved the goal. \n",
    "    for dom_k, dom_v  in d['goal'].items():\n",
    "        if dom_v and dom_k not in ignore_keys_in_goal: # check whether contains some goal entities\n",
    "            domains.append(dom_k)\n",
    "            \n",
    "    lines = []\n",
    "    \n",
    "    prev_d = None\n",
    "    for i, t in enumerate(d['log']):\n",
    "        spk = 'Usr' if i % 2 == 0 else 'Sys' # Turn 0 is always a user's turn in this corpus.\n",
    "        if spk == 'Sys':\n",
    "            if prev_d is None:\n",
    "                prev_d = t['metadata']\n",
    "            else:\n",
    "                crnt_d = t['metadata']\n",
    "                dst_diff = get_dst_diff(prev_d, crnt_d)\n",
    "                prev_d = crnt_d\n",
    "\n",
    "        lines.append(text_tokens(t['text']))\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array(data):\n",
    "    # extracting dialogs from data\n",
    "    dialogs = [get_lines(dialog) for dialog in data]\n",
    "\n",
    "    s = []\n",
    "    # extracting individual dialogs\n",
    "    for sentences in dialogs:\n",
    "        # extracting each conversation turn\n",
    "        for sentence in sentences: \n",
    "            s.append(sentence)\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of individual conversation turns from MultiWOZ dataset\n",
    "data_full = to_array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # counting the occurrences of words in the dataset\n",
    "\n",
    "# creating a vocabulary of top 3000 most frequent words in the dataset\n",
    "vocab = []\n",
    "for sentence in data_full:\n",
    "    for word in sentence:\n",
    "        vocab.append(word)\n",
    "\n",
    "VOCAB_SIZE=3000 # constant for the vocabulary size\n",
    "\n",
    "d = Counter(vocab) # count number of occurrences\n",
    "q = d.most_common()[0:VOCAB_SIZE] # create vocabulary of VOCAB_SIZE most common words\n",
    "vocab_top = [q[i][0] for i in range(VOCAB_SIZE)] # extracting the words from Counter structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# From Deep Learning for Text Processing Workshop at Machine Learning Prague 2018\n",
    "# https://github.com/rossumai/mlprague18-nlp\n",
    "\n",
    "# Creating dictionary of each word in the pre-trained GloVe embeddings, saving its location indexes \n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "GLOVE_DIR = \"../glove\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.%dd.txt' % EMBEDDING_DIM))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a matrix that is indexed by our vocabulary, containing GloVe embedding for each vocabulary element\n",
    "embedding_matrix = np.zeros((len(vocab_top) + 1, EMBEDDING_DIM))\n",
    "for i, word in enumerate(vocab_top):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros; also, [0] is reserved for padding.\n",
    "        embedding_matrix[i + 1] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_indices_vector(tokens):\n",
    "    vector = [0] * seq_cutoff\n",
    "    \n",
    "    for i, t in enumerate(tokens):\n",
    "        try:\n",
    "            vector[i] = vocab_top.index(t) + 1 # reserving 0 for padding\n",
    "        except:\n",
    "            pass # ignore missing words\n",
    "        \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(data):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(len(data)):\n",
    "        # first half of the dataset is made out of interrupted conversation turns \n",
    "        if (len(data[i]) > 4) and (i <= len(data)//2):\n",
    "            # picking random point for splitting the conversation turn\n",
    "            l = random.randrange(1, len(data[i]) - 3)\n",
    "            # splitting data\n",
    "            X.append(data[i][:l])\n",
    "            # adding 0 to the target list -> 0 -- interrupted turn \n",
    "            Y.append(0)\n",
    "                \n",
    "        # second half of the dataset is made out of full conversation turns\n",
    "        else:\n",
    "            # adding the full uninterrupted conversation turn\n",
    "            X.append(data[i])\n",
    "            # adding 1 to the target list -> 1 -- uninterrupted turn \n",
    "            Y.append(1)\n",
    "            \n",
    "    # shuffling the dataset\n",
    "    c = list(zip(X, Y, data))\n",
    "    random.shuffle(c)\n",
    "    X, Y, data = zip(*c)\n",
    "    \n",
    "    \n",
    "    # vectorizing the data\n",
    "    X = [vocab_indices_vector(x) for x in X]\n",
    "    \n",
    "    return X,Y,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest: 61, Average: 12.765652, Median: 12, 90% percentile: 61 tokens\n"
     ]
    }
   ],
   "source": [
    "lengths = sorted([len(x) for x in data_full]) # sorted lengths of the all reviews\n",
    "percentile = 0.90 # we are looking at reviews that are short, the 10% making the long reviews\n",
    "seq_cutoff = lengths[-1]\n",
    "print(\n",
    "    'Longest: %d, Average: %f, Median: %d, %d%% percentile: %d tokens' % \n",
    "    (lengths[-1], np.mean(lengths), lengths[int(len(lengths)*0.5)], percentile*100, seq_cutoff)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002371172240746815"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratio of words not included in the GloVe matrix\n",
    "1. * np.count_nonzero(np.all(embedding_matrix == 0, axis=1)) / len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y, data_clean = split_data(data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and validating parts\n",
    "# 2/3 for training, 1/3 for validating\n",
    "s = (len(X)//3)*2\n",
    "\n",
    "X_train = X[:s]\n",
    "Y_train = Y[:s]\n",
    "\n",
    "X_val = X[s+1:]\n",
    "Y_val = Y[s+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "seq_input (InputLayer)       [(None, 61)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 61, 50)            150050    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 61, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 59, 256)           38656     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 254,755\n",
      "Trainable params: 254,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Fitting...\n",
      "Train on 95364 samples, validate on 47683 samples\n",
      "Epoch 1/7\n",
      "95364/95364 [==============================] - 50s 528us/sample - loss: 0.2903 - accuracy: 0.8765 - val_loss: 0.2370 - val_accuracy: 0.9050\n",
      "Epoch 2/7\n",
      "95364/95364 [==============================] - 50s 523us/sample - loss: 0.2270 - accuracy: 0.9109 - val_loss: 0.2317 - val_accuracy: 0.9061\n",
      "Epoch 3/7\n",
      "95364/95364 [==============================] - 65s 681us/sample - loss: 0.2085 - accuracy: 0.9169 - val_loss: 0.2348 - val_accuracy: 0.9074\n",
      "Epoch 4/7\n",
      "95364/95364 [==============================] - 73s 767us/sample - loss: 0.1972 - accuracy: 0.9218 - val_loss: 0.2245 - val_accuracy: 0.9133\n",
      "Epoch 5/7\n",
      "95364/95364 [==============================] - 70s 737us/sample - loss: 0.1867 - accuracy: 0.9260 - val_loss: 0.2235 - val_accuracy: 0.9141\n",
      "Epoch 6/7\n",
      "95364/95364 [==============================] - 56s 586us/sample - loss: 0.1785 - accuracy: 0.9302 - val_loss: 0.2342 - val_accuracy: 0.9112\n",
      "Epoch 7/7\n",
      "95364/95364 [==============================] - 68s 708us/sample - loss: 0.1697 - accuracy: 0.9333 - val_loss: 0.2365 - val_accuracy: 0.9109\n"
     ]
    }
   ],
   "source": [
    "# NN from Deep Learning for Text Processing Workshop at Machine Learning Prague 2018\n",
    "# https://github.com/rossumai/mlprague18-nlp\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Conv1D, Dense, Embedding, GlobalMaxPooling1D, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "class GloveCNNAwesomeSentimentModel(object):\n",
    "    def __init__(self, N=256, size=3):\n",
    "        self.model = self.create(N, size)\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    def create(self, N, size):\n",
    "        seq_indices = Input(shape=(seq_cutoff,), name='seq_input') \n",
    "        seq_embedded = Embedding(input_dim=len(vocab_top) + 1, output_dim=EMBEDDING_DIM,\n",
    "                                 input_length=seq_cutoff)(seq_indices)\n",
    "        seq_conv = Conv1D(N, size, activation='relu')(Dropout(0.2)(seq_embedded)) # dropout - 0.2\n",
    "        max_conv = GlobalMaxPooling1D()(seq_conv)\n",
    "        hidden_repr = Dense(N, activation='relu')(max_conv)\n",
    "        sentiment = Dense(1, activation='sigmoid')(Dropout(0.2)(hidden_repr))\n",
    "\n",
    "        return Model(inputs=[seq_indices], outputs=[sentiment])\n",
    "\n",
    "    def train(self, X, y, X_val, y_val):\n",
    "        print('Fitting...')\n",
    "        return self.model.fit(np.array(X), np.array(y), \n",
    "                              validation_data=(np.array(X_val), np.array(y_val)),\n",
    "                              epochs=7, verbose=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(np.array(X))\n",
    "    \n",
    "sentiment = GloveCNNAwesomeSentimentModel()\n",
    "history = sentiment.train(\n",
    "    X_train, Y_train, X_val, Y_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.9977252] no that's ok how about colleges can you recommend a good one in the centre of town\n",
      "1 [0.7784515] i am actually leaving from cote\n",
      "1 [0.90955937] the tr arrives at will this be okay\n",
      "1 [0.9627397] certainly what is your departure location and your arrival location\n",
      "0 [0.8038908] that is all for now\n",
      "0 [0.99434185] i found restaurants please tell me what type of food you like to help narrow the search\n",
      "0 [0.0003536] no but could you give me\n",
      "1 [0.99978024] have a lovely day goodbye\n",
      "0 [0.2711899] your contact number\n"
     ]
    }
   ],
   "source": [
    "# test of the network\n",
    "for i in range(200,210):\n",
    "    if np.random.uniform() > 0.5:\n",
    "        if (len(data_clean[i]) > 4):\n",
    "            # picking random point for splitting the conversation turn\n",
    "            l = random.randrange(1, len(data_clean[i]) - 3)\n",
    "\n",
    "            test_tokens = text_tokens(' '.join(str(e) for e in data_clean[i][:l]))\n",
    "            prediction = sentiment.predict([vocab_indices_vector(test_tokens)])[0]\n",
    "\n",
    "            print(0, prediction, ' '.join(str(e) for e in data_clean[i][:l]))\n",
    "    else:\n",
    "        test_tokens = text_tokens(' '.join(str(e) for e in data_clean[i]))\n",
    "        prediction = sentiment.predict([vocab_indices_vector(test_tokens)])[0]\n",
    "\n",
    "        print(1, prediction, ' '.join(str(e) for e in data_clean[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2454347674548626, 0.9069]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of the network\n",
    "sentiment.model.evaluate(X_val[10000:20000], Y_val[10000:20000], verbose=0, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
